# SOW: 「有料noteを"スキ（いいね）数"で並べ替えて収集するシンプル調査ツール」

────────────────────────────────────
## 0. 目的 / ゴール
────────────────────────────────────
- note上の公開記事から「有料記事のみ」を抽出し、スキ数（like_count）の降順で一覧化する。
- 個人利用を前提に、StreamlitベースのWebアプリとして実装する。
- 非稼ぐ系の調査効率を上げるため、キーワード指定＋除外ワードで絞り込めるようにする。

成果物:
- Streamlit Webアプリケーション（ローカル実行＋Streamlit Cloudデプロイ対応）
- CSVファイルダウンロード機能
  - 列: likes, price, title, url, author_urlname, publish_at, description_short
- 検索結果の画面表示（テーブル形式）

────────────────────────────────────
## 1. 非機能要件 / 方針
────────────────────────────────────
- 個人用途・低頻度の実行を想定。大量アクセスは禁止。
- 仕様変更に備え、失敗時は graceful にスキップしログへ記録。
- レート制御: 1リクエスト毎に 800〜1200ms の待機（ジッターあり）。
- 取得上限: 初期値 200件（= 20件 × 10ページ）を上限に、設定で可変。
- 文字コード: UTF-8。
- タイムゾーン: Asia/Tokyo。日時はISO8601文字列を保持。

────────────────────────────────────
## 2. 実行イメージ（Streamlit版）
────────────────────────────────────
- 想定実装: StreamlitベースのWebアプリケーション
- 実行方法:
  - ローカル: `streamlit run app.py`
  - オンライン: Streamlit Cloud経由でアクセス

- UI構成:
  - 検索キーワード入力欄
  - フィルタープリセット選択（ドロップダウン）
  - カスタム除外ワード入力（オプション）
  - 検索実行ボタン
  - 結果表示テーブル
  - CSVダウンロードボタン

- 出力:
  - 画面上に検索結果テーブル表示
  - CSVファイルダウンロード（ブラウザ経由）

────────────────────────────────────
## 3. 主要機能
────────────────────────────────────
### 3.1 検索収集
- エンドポイント: /api/v3/searches?context=note&q={kw}&size={size}&start={start}
- ページング:
  - size: 20 固定（推奨）
  - start: 0, 20, 40, … と加算
- 取得配列の場所: data.notes.contents（v3）
- 必要フィールド（想定）:
  - name（タイトル）
  - description（概要）※無い場合あり
  - price（価格。>0 で有料）
  - like_count（スキ数）
  - key（記事キー）
  - publish_at（公開日時）
  - user.urlname（著者URL名）
  - eyecatch / thumbnail_external_url（任意）

### 3.2 フィルタリング
- 有料判定: price > 0
- 除外ワード: タイトル name と 概要 description に対し、部分一致で除外（大小区別なし）
  - 例: ["稼", "副業", "アフィ", "収益", "ビジネス"]
- 追加フィルタ（任意のON/OFF設定）:
  - 公開日範囲: publish_at ∈ [from, to]
  - 最低スキ数: like_count >= N
  - 価格上限: price <= M

### 3.3 並べ替え・出力
- ソート: like_count DESC, publish_at DESC（同率タイの場合）
- 表示項目:
  1) likes（スキ数）
  2) price（価格）
  3) title（タイトル - クリックでnoteページへ）
  4) author（著者名）
  5) publish_at（公開日）
  6) description_short（概要 - 100字に丸め）
- CSV出力時の列順:
  1) likes
  2) price
  3) title
  4) url
  5) author_urlname
  6) publish_at
  7) description_short
- エンコーディング: UTF-8（BOMなし）

### 3.4 エラーハンドリング（Streamlit版）
- ユーザー向けエラー表示:
  - st.error() でわかりやすいメッセージ表示
  - 部分的な結果がある場合は表示し、エラー内容を通知
- 内部ログ:
  - st.sidebar の詳細情報エリアに表示（オプション）
  - HTTPステータス, URL, 再試行回数
- 429/403/5xx: バックオフ（指数 + ジッター）で最大3回まで再試行

### 3.5 進捗表示
- st.progress() で取得進捗をリアルタイム表示
- st.spinner() で処理中メッセージ表示
- 取得済み記事数のカウンター表示

────────────────────────────────────
## 4. 設定仕様（Streamlit版）
────────────────────────────────────
### 4.1 UI設定項目
- 検索キーワード（テキスト入力）
- フィルタープリセット（選択式）:
  - 「ゆるめ」: 明確なビジネス系のみ除外
  - 「標準」: 一般的なビジネス関連ワード除外
  - 「厳しめ」: 創作系に特化
  - 「カスタム」: ユーザー定義
- 詳細設定（エキスパンダー内）:
  - 取得ページ数（スライダー: 1-20）
  - 価格上限（数値入力）
  - 最低いいね数（数値入力）
  - カスタム除外ワード（テキストエリア）

### 4.2 フィルタープリセット定義
```python
FILTER_PRESETS = {
    "ゆるめ": {
        "exclude_words": ["稼ぐ", "稼げる", "副業で月収"],
        "price_max": None
    },
    "標準": {
        "exclude_words": ["稼", "副業", "収益", "ビジネス", "マネタイズ"],
        "price_max": 5000
    },
    "厳しめ": {
        "exclude_words": ["稼", "副業", "ビジネス", "収益", "集客", "マーケ", "売上", "コンサル"],
        "price_max": 3000
    }
}
```

────────────────────────────────────
## 5. 正常・異常系
────────────────────────────────────
### 5.1 正常
- 各ページの取得成功 → フィルタ → ソート → CSV保存 → 終了コード 0

### 5.2 異常（代表例とハンドリング）
- **400 Bad Request**:
  - クエリやパラメータ不正。実行を停止し、ユーザーへ設定見直しを促すメッセージを出す。
- **403 Forbidden**:
  - 制限超過・ブロックの可能性。バックオフ再試行。失敗時は中断し、取得済み分でCSVを保存。
- **404 Not Found**:
  - エンドポイントまたは記事キー不正。スキップして継続。
- **429 Too Many Requests**:
  - レート制限。待機時間を増やして再試行。限界超過で中断。
- **5xx**:
  - サーバ側エラー。指数バックオフで再試行。最終失敗時は中断し、部分結果を保存。

終了コード:
- 0: 正常終了
- 2: 一部失敗（部分結果保存済み）
- 9: 重大失敗（出力なし）

────────────────────────────────────
## 6. テスト観点（最低限）
────────────────────────────────────
- 単一ページ取得でCSV 20行（ヘッダー除く）が出力される。
- price=0 の記事が1件も含まれない。
- exclude_words に含めた語が title/description に含まれる記事が除外される。
- like_count の降順で並ぶ（同率は publish_at 降順）。
- pages を 0/負数 指定時はエラー扱い。
- 429/403 時に待機→再試行する。
- ネットワーク断でもエラーメッセージを返し、プロセスがハングしない。
- 同条件で2回実行した場合、キャッシュが有効なら2回目は高速に完了する（任意）。

────────────────────────────────────
## 7. 仕様変更に備えた実装メモ
────────────────────────────────────
- JSONパスは固定せず、「data.notes.contents」が無い場合は全体JSONをログ出力し、自動検出（候補: data.contents / data / notes）。
- like_count / price などのフィールド名変更に備え、フィールドマッピングを設定化可能にする。
- 取得不能時のフォールバック（例: creators APIで著者単位の取得を試す）は今回は実装しない（シンプル優先）。必要時に拡張。

────────────────────────────────────
## 8. 今後の拡張（任意・後回し）
────────────────────────────────────
- 検索履歴機能: 過去の検索条件を保存・再利用
- 結果のフィルタリング: 取得後の追加フィルタ（価格帯、日付範囲等）
- グラフ表示: いいね数分布、価格帯分析等の可視化
- ハッシュタグ横断: /api/v2/hashtags と組み合わせてトレンド→記事収集
- 定期実行: Streamlit Cloudのスケジュール機能活用
- お気に入り機能: 特定の記事をマーク・後で確認

────────────────────────────────────
## 9. コンプライアンス / 運用上の注意
────────────────────────────────────
- noteの robots.txt は一般クローラーに対して /api/* を Disallow。個人利用・低頻度での取得に限定し、公開クローラー的な大量収集は不可。アクセス頻度は十分に落とすこと。将来仕様が変わる可能性を常に考慮する。
- 非公式APIのため、予告なく項目名や構造が変わる可能性あり。失敗時に完全停止せず、部分結果を保存できる実装にする。
- 収集データの再頒布・商用提供は避け、個人の調査用途に限定する。

────────────────────────────────────
## 10. 参考情報・出典
────────────────────────────────────
### note 非公式API仕様
- **API一覧とv3 /searches仕様**
  - https://note.com/ego_station/n/n85fcb635c0a9
  - v3 /searches、size/start ページング、data.notes.contents 内に price・like_count 等が含まれる例

- **主要エンドポイントの整理**
  - https://note.com/soh_ainsight/n/n80d43f9c467e
  - /v3/searches?context=note、/v2/creators/{urlname}/contents、/v3/notes/{key}、/v3/notes/{key}/likes

- **検索APIで取得できる要素**
  - https://note.com/manochi/n/n4f57e7ae7b9b
  - タイトル、作成者、公開日時、スキ数、サムネイル等の取得可能項目

### コンプライアンス関連
- **robots.txt 設定**
  - https://note.com/robots.txt
  - /api/* の Disallow 設定（一般UA）、Googlebot 例外の記載

- **非公式API利用上の注意**
  - https://note.com/manochi/n/n4f57e7ae7b9b
  - 仕様変更やアクセス頻度制限に留意すべきとの注意事項